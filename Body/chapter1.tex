\chapter{INTRODUCTION}
\label{chp:introduction}
This dissertation outlines a method to achieve high performance sparse matrix vector multiplication (SpMV) on the Convey HC-2ex. Although we target one specfic platform this work should port well to other FPGA platforms. In creating our solution, we developed several technologies that reach into adjacent domains, incuding: a new matrix traversal, a new multiply-accumulator, a new sparse matrix compression algorithm, a new floating point compressiong algoritm, and a multi-port memory core.

\par SpMV is used in a variety of applications including information retrieval [\cite{prelim:page}], text classification [\cite{prelim:townsend2}], and image processing [\cite{prelim:wang}]. Often, the SpMV operations are iterative or repetitive and require a large amount of computation. For example, the PageRank algorithm uses iterative SpMV for eigenvector estimation.

\par For the most part, modern CPUs compute SpMV well. In fact, most papers on the subject of computing SpMV on FPGAs show FPGAs have worse performance. This happens because current HPRC machines no where near the amount of memory bandwidth that current CPU and GPU machines have. The machine we use (the Convey HC2-ex) has only 19GB/s bandwidth per FPGA, whereas current CPUS have 100 GB/s and GPUs have 290 GB/s.

However, there is a small niche where FPGAs can excel, even with this handicap. Application that use repetitive SpMV operations on large matrices have a chance of performing the best on FPGAs. When the matrix and vector sizes  become large, around 10 million values, CPU performance drastically decreases. To address this issue most people turn to GPUs.
\par However, GPUs have an interesting characteristic. In order to achieve good performance GPUs expand the storage size of the matrix. FPGAs do the opposite and compress the size of the matrix. This means matrices with more than 400 million values perform badly or do not fit in the GPU's RAM.
\par GPUs are stuck between a rock and a hard place~[\cite{prelim:davis0}]. The rock being CPUs that compute SpMV on matrices with less than 10 million values well. The hard place being FPGAs that compute SpMV on matrices with more than 400 million values well (or at least not as badly as CPUs and GPUs).
\par In the Chapter~\ref{chp:background}, we describe the previous approaches to SpMV on CPUs, GPUs and FPGAs. In Chapters~\ref{chp:method}, \ref{chp:mac}, \ref{chp:compression}, \ref{chp:fzip}, and \ref{chp:memory}, we discuss our optimizations for FPGAs. In Chapter \ref{chp:high_level_design} we present our high level design and results. In Chapter \ref{chp:conclusion} we conclude the paper.
