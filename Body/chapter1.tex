\chapter{INTRODUCTION}
\label{chapter:introduction}
This preliminary report outlines a plan to double the current performance of sparse matrix vector multiplication on FPGA platforms.
\par People use SpMV in a variety of applications including information retrieval, text classification, scientific computing and image processing. Often the SpMV operations are iterative or repeatative and require a large amount of computation. Eigenvector estimation often uses iterative SpMV operations. For example, the pagerank algorithm uses SpMV for eigenvector estimation.
\par For the most part modern CPUs perform SpMV well. Some may even say FPGAs deserve no consideration when computing SpMV, or say that computing SpMV on FPGAs is solely academic and would only help to design better ASICs, CPUs and GPUs for computing SpMV. We disagree. If you have  an application that uses repetitive SpMV operations on large matrices then FPGAs are exactly the chips you should be looking at. When the matrix and vector sizes  become large, around 10 million values, CPU performance drastically decreases. To address this issue most people turn to GPUs.
\par However, GPUs have an interesting characteristic. In order to achieve good performance GPUs expand the storage size of the matrix. FPGAs do the opposite and compress the size of the matrix. This means matrices with more than 400 million values perform badly or do not fit in the GPU's RAM.
\par So GPUs are stuck between a rock and a hard place~[\cite{prelim:davis0}]. The rock being CPUs that compute SpMV on matrices with less than 10 million values well. The hard place being FPGAs that compute SpMV on matrices with more than 400 million values well (or at least not as badly as CPUs and GPUs).
\par In the chapter 2 we describe the previous approaches to SpMV on CPUs, GPUs and FPGAs. In chapters 3 to 7 we discuss our optimizations for FPGAs. In chapters 8 and 9 we present our predicted results and discuss the timeline to achieve these results.
