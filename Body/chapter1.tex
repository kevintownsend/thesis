\chapter{INTRODUCTION}
\label{chp:introduction}
This dissertation outlines a method to achieve high performance sparse matrix vector multiplication (SpMV) on the Convey HC-2ex. Although we target one specfic platform this work should port well to other FPGA platforms. In creating our solution, we developed several IPs that reach into other domains, incuding: a new matrix traversal, a new multiply-accumulator, a new sparse matrix compression algorithm, a new floating point compressiong algoritm, and a multi-port memory core.

\par People use SpMV in a variety of applications including information retrieval [\cite{prelim:page}], text classification [\cite{prelim:townsend2}], and image processing [\cite{prelim:wang}]. Often, the SpMV operations are iterative or repetitive and require a large amount of computation. Eigenvector estimation often uses iterative SpMV operations. For example, the PageRank algorithm uses interative SpMV for eigenvector estimation.

\par For the most part, modern CPUs compute SpMV well. In fact, most papers on the subject of computing SpMV on FPGAs show FPGAs have worse performance. This happens because current HPRC machines no where near the amount of memory bandwidth that current CPU and GPU machines have. The Convey HC2-ex has only 19GB/s bandwidth per FPGA, whereas current CPUS have 100 GB/s and GPUs have 200 GB/s. TODO: check. There is hope that HPRC machine will improve.

However, there is a small niche where FPGAs can excell, even with these handicaps. If you have  an application that uses repetitive SpMV operations on large matrices then FPGAs are exactly the chips you should be looking at. When the matrix and vector sizes  become large, around 10 million values, CPU performance drastically decreases. To address this issue most people turn to GPUs.
\par However, GPUs have an interesting characteristic. In order to achieve good performance GPUs expand the storage size of the matrix. FPGAs do the opposite and compress the size of the matrix. This means matrices with more than 400 million values perform badly or do not fit in the GPU's RAM.
\par So GPUs are stuck between a rock and a hard place~[\cite{prelim:davis0}]. The rock being CPUs that compute SpMV on matrices with less than 10 million values well. The hard place being FPGAs that compute SpMV on matrices with more than 400 million values well (or at least not as badly as CPUs and GPUs).
\par In the Chapter~\ref{chp:background}, we describe the previous approaches to SpMV on CPUs, GPUs and FPGAs. In Chapters~\ref{chp:method}, \ref{chp:mac}, \ref{chp:compression}, \ref{chp:fzip}, and \ref{chp:memory}, we discuss our optimizations for FPGAs. In Chapter \ref{chp:high_level_design} we present our high level design and results. In Chapter \ref{chp:conclusion} we conclude the paper.
